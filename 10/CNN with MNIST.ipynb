{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n",
      "Epoch 0\n",
      "tensor(2.3058)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshhu/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:64: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/joshhu/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:86: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/joshhu/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:90: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0978)\n",
      "tensor(1.8258)\n",
      "tensor(1.6781)\n",
      "tensor(0.7523)\n",
      "tensor(2.0277)\n",
      "tensor(0.6322)\n",
      "tensor(0.5482)\n",
      "tensor(0.6572)\n",
      "tensor(0.3610)\n",
      "tensor(0.3446)\n",
      "tensor(0.5306)\n",
      "tensor(0.4659)\n",
      "tensor(0.2169)\n",
      "tensor(0.9212)\n",
      "tensor(0.6201)\n",
      "tensor(0.4350)\n",
      "tensor(0.2834)\n",
      "tensor(0.8334)\n",
      "tensor(0.5853)\n",
      "tensor(0.2192)\n",
      "tensor(0.2087)\n",
      "tensor(0.6505)\n",
      "tensor(0.7470)\n",
      "tensor(0.3865)\n",
      "tensor(0.1194)\n",
      "tensor(0.2985)\n",
      "tensor(0.8105)\n",
      "tensor(0.3007)\n",
      "tensor(0.9042)\n",
      "tensor(0.1978)\n",
      "tensor(0.1868)\n",
      "tensor(0.2038)\n",
      "tensor(0.3340)\n",
      "tensor(0.2505)\n",
      "tensor(0.8807)\n",
      "tensor(0.3901)\n",
      "tensor(0.0998)\n",
      "tensor(0.3404)\n",
      "tensor(0.1472)\n",
      "tensor(0.2322)\n",
      "tensor(0.0521)\n",
      "tensor(0.1167)\n",
      "tensor(0.0837)\n",
      "tensor(0.0611)\n",
      "tensor(0.4273)\n",
      "tensor(0.0893)\n",
      "tensor(0.1625)\n",
      "tensor(0.0135)\n",
      "tensor(0.0325)\n",
      "tensor(0.3045)\n",
      "tensor(0.1555)\n",
      "tensor(0.2240)\n",
      "tensor(0.3185)\n",
      "tensor(0.7536)\n",
      "tensor(0.5232)\n",
      "tensor(0.1370)\n",
      "tensor(0.1561)\n",
      "tensor(2.0029)\n",
      "tensor(0.0731)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshhu/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:98: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/home/joshhu/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:102: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0945, Accuracy: 9703/10000 (97%)\n",
      "\n",
      "Epoch 1\n",
      "tensor(0.3067)\n",
      "tensor(1.2007)\n",
      "tensor(0.5318)\n",
      "tensor(0.3373)\n",
      "tensor(1.0636)\n",
      "tensor(0.3065)\n",
      "tensor(0.3004)\n",
      "tensor(1.8466)\n",
      "tensor(0.1624)\n",
      "tensor(0.8236)\n",
      "tensor(0.0167)\n",
      "tensor(0.1881)\n",
      "tensor(0.0568)\n",
      "tensor(0.2077)\n",
      "tensor(0.1726)\n",
      "tensor(1.6097)\n",
      "tensor(0.4424)\n",
      "tensor(0.5585)\n",
      "tensor(0.2492)\n",
      "tensor(1.1940)\n",
      "tensor(0.5910)\n",
      "tensor(0.0972)\n",
      "tensor(0.2928)\n",
      "tensor(0.0748)\n",
      "tensor(0.0572)\n",
      "tensor(0.8895)\n",
      "tensor(0.2555)\n",
      "tensor(0.1150)\n",
      "tensor(0.0762)\n",
      "tensor(0.2204)\n",
      "tensor(0.2585)\n",
      "tensor(0.1527)\n",
      "tensor(0.6118)\n",
      "tensor(0.3880)\n",
      "tensor(0.0105)\n",
      "tensor(0.7392)\n",
      "tensor(0.0932)\n",
      "tensor(0.3719)\n",
      "tensor(0.2051)\n",
      "tensor(0.1112)\n",
      "tensor(0.4832)\n",
      "tensor(0.2856)\n",
      "tensor(0.4440)\n",
      "tensor(0.0146)\n",
      "tensor(0.4678)\n",
      "tensor(0.0425)\n",
      "tensor(0.1710)\n",
      "tensor(0.0418)\n",
      "tensor(0.1276)\n",
      "tensor(2.1389)\n",
      "tensor(0.1519)\n",
      "tensor(0.1173)\n",
      "tensor(0.1584)\n",
      "tensor(0.1832)\n",
      "tensor(0.2981)\n",
      "tensor(0.0840)\n",
      "tensor(0.8388)\n",
      "tensor(0.0261)\n",
      "tensor(0.1998)\n",
      "tensor(0.9148)\n",
      "\n",
      "Test set: Average loss: 0.0672, Accuracy: 9788/10000 (97%)\n",
      "\n",
      "Epoch 2\n",
      "tensor(0.5733)\n",
      "tensor(0.0959)\n",
      "tensor(0.1453)\n",
      "tensor(0.4507)\n",
      "tensor(0.0621)\n",
      "tensor(0.1248)\n",
      "tensor(0.0669)\n",
      "tensor(0.3003)\n",
      "tensor(0.2658)\n",
      "tensor(0.0697)\n",
      "tensor(0.1087)\n",
      "tensor(0.0693)\n",
      "tensor(0.8649)\n",
      "tensor(0.0112)\n",
      "tensor(0.2678)\n",
      "tensor(0.7451)\n",
      "tensor(0.1598)\n",
      "tensor(0.4034)\n",
      "tensor(0.0408)\n",
      "tensor(0.2377)\n",
      "tensor(0.0232)\n",
      "tensor(0.1384)\n",
      "tensor(0.1019)\n",
      "tensor(0.0226)\n",
      "tensor(0.1241)\n",
      "tensor(0.0113)\n",
      "tensor(0.0109)\n",
      "tensor(0.1851)\n",
      "tensor(0.2045)\n",
      "tensor(0.0639)\n",
      "tensor(0.0594)\n",
      "tensor(0.0881)\n",
      "tensor(0.0959)\n",
      "tensor(0.0298)\n",
      "tensor(0.6997)\n",
      "tensor(1.0012)\n",
      "tensor(0.8890)\n",
      "tensor(0.0144)\n",
      "tensor(1.1205)\n",
      "tensor(0.3725)\n",
      "tensor(0.4399)\n",
      "tensor(0.3309)\n",
      "tensor(0.0088)\n",
      "tensor(0.1901)\n",
      "tensor(0.2321)\n",
      "tensor(0.1268)\n",
      "tensor(0.6011)\n",
      "tensor(0.0492)\n",
      "tensor(0.3702)\n",
      "tensor(0.2556)\n",
      "tensor(0.0110)\n",
      "tensor(0.1089)\n",
      "tensor(0.0843)\n",
      "tensor(0.0649)\n",
      "tensor(0.2130)\n",
      "tensor(0.0136)\n",
      "tensor(0.0196)\n",
      "tensor(0.0357)\n",
      "tensor(0.1071)\n",
      "tensor(0.1289)\n",
      "\n",
      "Test set: Average loss: 0.0589, Accuracy: 9816/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# download and transform train dataset\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          train=True,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# download and transform test dataset\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          train=False,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    \"\"\"Custom module for a simple convnet classifier\"\"\"\n",
    "    def __init__(self):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input is 28x28x1\n",
    "        # conv1(kernel=5, filters=10) 28x28x10 -> 24x24x10\n",
    "        # max_pool(kernel=2) 24x24x10 -> 12x12x10\n",
    "        \n",
    "        # Do not be afraid of F's - those are just functional wrappers for modules form nn package\n",
    "        # Please, see for yourself - http://pytorch.org/docs/_modules/torch/nn/functional.html\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        \n",
    "        # conv2(kernel=5, filters=20) 12x12x20 -> 8x8x20\n",
    "        # max_pool(kernel=2) 8x8x20 -> 4x4x20\n",
    "        x = F.relu(F.max_pool2d(self.dropout(self.conv2(x)), 2))\n",
    "        \n",
    "        # flatten 4x4x20 = 320\n",
    "        x = x.view(-1, 320)\n",
    "        \n",
    "        # 320 -> 50\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        # 50 -> 10\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # transform to logits\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "# create classifier and optimizer objects\n",
    "clf = CNNClassifier()\n",
    "opt = optim.SGD(clf.parameters(), lr=0.01, momentum=0.5)\n",
    "\n",
    "loss_history = []\n",
    "acc_history = []\n",
    "\n",
    "def train(epoch):\n",
    "    clf.train() # set model in training mode (need this because of dropout)\n",
    "    \n",
    "    # dataset API gives us pythonic batching \n",
    "    for batch_id, (data, label) in enumerate(train_loader):\n",
    "        data = Variable(data)\n",
    "        target = Variable(label)\n",
    "        \n",
    "        # forward pass, calculate loss and backprop!\n",
    "        opt.zero_grad()\n",
    "        preds = clf(data)\n",
    "        loss = F.nll_loss(preds, target)\n",
    "        loss.backward()\n",
    "        loss_history.append(loss.data[0])\n",
    "        opt.step()\n",
    "        \n",
    "        if batch_id % 100 == 0:\n",
    "            print(loss.data[0])\n",
    "\n",
    "def test(epoch):\n",
    "    clf.eval() # set model in inference mode (need this because of dropout)\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        data = Variable(data, volatile=True) \n",
    "        target = Variable(target)\n",
    "        \n",
    "        output = clf(data)\n",
    "        test_loss += F.nll_loss(output, target).data[0]\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    acc_history.append(accuracy)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        accuracy))\n",
    "\n",
    "for epoch in range(0, 3):\n",
    "    print(\"Epoch %d\" % epoch)\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
