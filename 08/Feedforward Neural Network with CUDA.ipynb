{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784       # The image size = 28 x 28 = 784\n",
    "hidden_size = 1000      # The number of nodes at the hidden layer\n",
    "num_classes = 10       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 10         # The number of times entire dataset is trained\n",
    "batch_size = 100       # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the loading process of train_dataset to make the learning process independent of data orderness, but the order of test_loader remains to examine whether we can handle unspecified bias order of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4, \n",
    "                                          pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4, \n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]\n",
      " - Numpy Shape: (60000, 28, 28)\n",
      " - Tensor Shape: torch.Size([60000, 28, 28])\n",
      " - min: tensor(0.)\n",
      " - max: tensor(1.)\n",
      " - mean: tensor(0.1307)\n",
      " - std: tensor(0.3081)\n",
      " - var: tensor(0.0949)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_dataset.train_data\n",
    "train_data = train_dataset.transform(train_data.numpy())\n",
    "\n",
    "print('[Train]')\n",
    "print(' - Numpy Shape:', train_dataset.train_data.cpu().numpy().shape)\n",
    "print(' - Tensor Shape:', train_dataset.train_data.size())\n",
    "print(' - min:', torch.min(train_data))\n",
    "print(' - max:', torch.max(train_data))\n",
    "print(' - mean:', torch.mean(train_data))\n",
    "print(' - std:', torch.std(train_data))\n",
    "print(' - var:', torch.var(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "        self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "    \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=1000, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshhu/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:14: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/joshhu/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:20: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/600], Loss: 0.2857\n",
      "Epoch [1/10], Step [200/600], Loss: 0.2238\n",
      "Epoch [1/10], Step [300/600], Loss: 0.2738\n",
      "Epoch [1/10], Step [400/600], Loss: 0.0585\n",
      "Epoch [1/10], Step [500/600], Loss: 0.1737\n",
      "Epoch [1/10], Step [600/600], Loss: 0.1154\n",
      "Epoch [2/10], Step [100/600], Loss: 0.1489\n",
      "Epoch [2/10], Step [200/600], Loss: 0.1863\n",
      "Epoch [2/10], Step [300/600], Loss: 0.0921\n",
      "Epoch [2/10], Step [400/600], Loss: 0.1833\n",
      "Epoch [2/10], Step [500/600], Loss: 0.0459\n",
      "Epoch [2/10], Step [600/600], Loss: 0.1061\n",
      "Epoch [3/10], Step [100/600], Loss: 0.1496\n",
      "Epoch [3/10], Step [200/600], Loss: 0.0505\n",
      "Epoch [3/10], Step [300/600], Loss: 0.0569\n",
      "Epoch [3/10], Step [400/600], Loss: 0.0447\n",
      "Epoch [3/10], Step [500/600], Loss: 0.0415\n",
      "Epoch [3/10], Step [600/600], Loss: 0.0483\n",
      "Epoch [4/10], Step [100/600], Loss: 0.0280\n",
      "Epoch [4/10], Step [200/600], Loss: 0.0225\n",
      "Epoch [4/10], Step [300/600], Loss: 0.0286\n",
      "Epoch [4/10], Step [400/600], Loss: 0.0081\n",
      "Epoch [4/10], Step [500/600], Loss: 0.0347\n",
      "Epoch [4/10], Step [600/600], Loss: 0.0655\n",
      "Epoch [5/10], Step [100/600], Loss: 0.0207\n",
      "Epoch [5/10], Step [200/600], Loss: 0.0085\n",
      "Epoch [5/10], Step [300/600], Loss: 0.0150\n",
      "Epoch [5/10], Step [400/600], Loss: 0.0240\n",
      "Epoch [5/10], Step [500/600], Loss: 0.0176\n",
      "Epoch [5/10], Step [600/600], Loss: 0.0383\n",
      "Epoch [6/10], Step [100/600], Loss: 0.0275\n",
      "Epoch [6/10], Step [200/600], Loss: 0.0658\n",
      "Epoch [6/10], Step [300/600], Loss: 0.0252\n",
      "Epoch [6/10], Step [400/600], Loss: 0.0620\n",
      "Epoch [6/10], Step [500/600], Loss: 0.0254\n",
      "Epoch [6/10], Step [600/600], Loss: 0.0096\n",
      "Epoch [7/10], Step [100/600], Loss: 0.0082\n",
      "Epoch [7/10], Step [200/600], Loss: 0.0094\n",
      "Epoch [7/10], Step [300/600], Loss: 0.0272\n",
      "Epoch [7/10], Step [400/600], Loss: 0.0391\n",
      "Epoch [7/10], Step [500/600], Loss: 0.0309\n",
      "Epoch [7/10], Step [600/600], Loss: 0.0490\n",
      "Epoch [8/10], Step [100/600], Loss: 0.0038\n",
      "Epoch [8/10], Step [200/600], Loss: 0.0048\n",
      "Epoch [8/10], Step [300/600], Loss: 0.0090\n",
      "Epoch [8/10], Step [400/600], Loss: 0.0040\n",
      "Epoch [8/10], Step [500/600], Loss: 0.0087\n",
      "Epoch [8/10], Step [600/600], Loss: 0.0020\n",
      "Epoch [9/10], Step [100/600], Loss: 0.0031\n",
      "Epoch [9/10], Step [200/600], Loss: 0.0013\n",
      "Epoch [9/10], Step [300/600], Loss: 0.0025\n",
      "Epoch [9/10], Step [400/600], Loss: 0.0027\n",
      "Epoch [9/10], Step [500/600], Loss: 0.0027\n",
      "Epoch [9/10], Step [600/600], Loss: 0.0079\n",
      "Epoch [10/10], Step [100/600], Loss: 0.0200\n",
      "Epoch [10/10], Step [200/600], Loss: 0.0012\n",
      "Epoch [10/10], Step [300/600], Loss: 0.0056\n",
      "Epoch [10/10], Step [400/600], Loss: 0.0026\n",
      "Epoch [10/10], Step [500/600], Loss: 0.0011\n",
      "Epoch [10/10], Step [600/600], Loss: 0.0119\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):   # Load a batch of images with its (index, data, class)\n",
    "        images = torch.FloatTensor(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = torch.LongTensor(labels)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = net(images)                             # Forward pass: compute the output class given a image\n",
    "        loss = criterion(outputs, labels)                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        losses.append(loss.cpu().data[0])\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "        \n",
    "        if (i+1) % 100 == 0:                              # Logging\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc1fad5e828>]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the FNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to training the nerual network, we also need to load batches of test images and collect the outputs. The differences are that:\n",
    "(1) No loss & weights calculation\n",
    "(2) No wights update\n",
    "(3) Has correct prediction calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10K test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = torch.FloatTensor(images.view(-1, 28*28))\n",
    "    \n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "    total += labels.size(0)                    # Increment the total count\n",
    "    correct += (predicted == labels).sum()     # Increment the correct count\n",
    "    \n",
    "print('Accuracy of the network on the 10K test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained FNN Model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(net.state_dict(), 'fnn_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
