{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784       # The image size = 28 x 28 = 784\n",
    "hidden_size = 1000      # The number of nodes at the hidden layer\n",
    "num_classes = 10       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 10         # The number of times entire dataset is trained\n",
    "batch_size = 100       # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the loading process of train_dataset to make the learning process independent of data orderness, but the order of test_loader remains to examine whether we can handle unspecified bias order of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=4, \n",
    "                                          pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=4, \n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train]\n",
      " - Numpy Shape: (60000, 28, 28)\n",
      " - Tensor Shape: torch.Size([60000, 28, 28])\n",
      " - min: tensor(0.)\n",
      " - max: tensor(1.)\n",
      " - mean: tensor(0.1307)\n",
      " - std: tensor(0.3081)\n",
      " - var: tensor(0.0949)\n"
     ]
    }
   ],
   "source": [
    "train_data = train_dataset.train_data\n",
    "train_data = train_dataset.transform(train_data.numpy())\n",
    "\n",
    "print('[Train]')\n",
    "print(' - Numpy Shape:', train_dataset.train_data.cpu().numpy().shape)\n",
    "print(' - Tensor Shape:', train_dataset.train_data.size())\n",
    "print(' - min:', torch.min(train_data))\n",
    "print(' - max:', torch.max(train_data))\n",
    "print(' - mean:', torch.mean(train_data))\n",
    "print(' - std:', torch.std(train_data))\n",
    "print(' - var:', torch.var(train_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "        \n",
    "        self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "    \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=784, out_features=1000, bias=True)\n",
       "  (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
       "  (relu): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshhu/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:14: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/joshhu/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:20: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/600], Loss: 0.3215\n",
      "Epoch [1/10], Step [200/600], Loss: 0.2004\n",
      "Epoch [1/10], Step [300/600], Loss: 0.3748\n",
      "Epoch [1/10], Step [400/600], Loss: 0.1253\n",
      "Epoch [1/10], Step [500/600], Loss: 0.1364\n",
      "Epoch [1/10], Step [600/600], Loss: 0.2117\n",
      "Epoch [2/10], Step [100/600], Loss: 0.0609\n",
      "Epoch [2/10], Step [200/600], Loss: 0.1474\n",
      "Epoch [2/10], Step [300/600], Loss: 0.0733\n",
      "Epoch [2/10], Step [400/600], Loss: 0.1424\n",
      "Epoch [2/10], Step [500/600], Loss: 0.1100\n",
      "Epoch [2/10], Step [600/600], Loss: 0.0862\n",
      "Epoch [3/10], Step [100/600], Loss: 0.0203\n",
      "Epoch [3/10], Step [200/600], Loss: 0.0504\n",
      "Epoch [3/10], Step [300/600], Loss: 0.0355\n",
      "Epoch [3/10], Step [400/600], Loss: 0.0187\n",
      "Epoch [3/10], Step [500/600], Loss: 0.0457\n",
      "Epoch [3/10], Step [600/600], Loss: 0.0531\n",
      "Epoch [4/10], Step [100/600], Loss: 0.0424\n",
      "Epoch [4/10], Step [200/600], Loss: 0.0711\n",
      "Epoch [4/10], Step [300/600], Loss: 0.0151\n",
      "Epoch [4/10], Step [400/600], Loss: 0.1573\n",
      "Epoch [4/10], Step [500/600], Loss: 0.0212\n",
      "Epoch [4/10], Step [600/600], Loss: 0.0253\n",
      "Epoch [5/10], Step [100/600], Loss: 0.0851\n",
      "Epoch [5/10], Step [200/600], Loss: 0.0214\n",
      "Epoch [5/10], Step [300/600], Loss: 0.0634\n",
      "Epoch [5/10], Step [400/600], Loss: 0.0335\n",
      "Epoch [5/10], Step [500/600], Loss: 0.0300\n",
      "Epoch [5/10], Step [600/600], Loss: 0.0129\n",
      "Epoch [6/10], Step [100/600], Loss: 0.0127\n",
      "Epoch [6/10], Step [200/600], Loss: 0.0150\n",
      "Epoch [6/10], Step [300/600], Loss: 0.0016\n",
      "Epoch [6/10], Step [400/600], Loss: 0.0067\n",
      "Epoch [6/10], Step [500/600], Loss: 0.0036\n",
      "Epoch [6/10], Step [600/600], Loss: 0.0263\n",
      "Epoch [7/10], Step [100/600], Loss: 0.0411\n",
      "Epoch [7/10], Step [200/600], Loss: 0.0089\n",
      "Epoch [7/10], Step [300/600], Loss: 0.0169\n",
      "Epoch [7/10], Step [400/600], Loss: 0.0331\n",
      "Epoch [7/10], Step [500/600], Loss: 0.0761\n",
      "Epoch [7/10], Step [600/600], Loss: 0.0226\n",
      "Epoch [8/10], Step [100/600], Loss: 0.0513\n",
      "Epoch [8/10], Step [200/600], Loss: 0.0034\n",
      "Epoch [8/10], Step [300/600], Loss: 0.0053\n",
      "Epoch [8/10], Step [400/600], Loss: 0.0053\n",
      "Epoch [8/10], Step [500/600], Loss: 0.0046\n",
      "Epoch [8/10], Step [600/600], Loss: 0.0224\n",
      "Epoch [9/10], Step [100/600], Loss: 0.0005\n",
      "Epoch [9/10], Step [200/600], Loss: 0.0076\n",
      "Epoch [9/10], Step [300/600], Loss: 0.0062\n",
      "Epoch [9/10], Step [400/600], Loss: 0.0016\n",
      "Epoch [9/10], Step [500/600], Loss: 0.0289\n",
      "Epoch [9/10], Step [600/600], Loss: 0.0013\n",
      "Epoch [10/10], Step [100/600], Loss: 0.0091\n",
      "Epoch [10/10], Step [200/600], Loss: 0.0025\n",
      "Epoch [10/10], Step [300/600], Loss: 0.0030\n",
      "Epoch [10/10], Step [400/600], Loss: 0.0156\n",
      "Epoch [10/10], Step [500/600], Loss: 0.0073\n",
      "Epoch [10/10], Step [600/600], Loss: 0.0141\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):   # Load a batch of images with its (index, data, class)\n",
    "        images = torch.FloatTensor(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = torch.LongTensor(labels)\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        \n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = net(images)                             # Forward pass: compute the output class given a image\n",
    "        loss = criterion(outputs, labels)                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        losses.append(loss.cpu().data[0])\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "        \n",
    "        if (i+1) % 100 == 0:                              # Logging\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd14aa75780>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl8VNXdx/HPLyRssgoRkcVgRRF3pOJW61KtUqtdtEWtex+fp4928+kCLti6VG3VtqhVcan7gkoVAQUUFFBAwr4JCWvCloQlQEL28/wxN8NMMhshyeQO3/frldfMvXPm3nNw/M6dc88915xziIhIaklLdgVERKTxKdxFRFKQwl1EJAUp3EVEUpDCXUQkBSncRURSkMJdRCQFKdxFRFKQwl1EJAWlJ2vH3bt3d1lZWcnavYiIL82bN6/IOZcZr1zSwj0rK4vs7Oxk7V5ExJfMbH0i5dQtIyKSghTuIiIpSOEuIpKCFO4iIilI4S4ikoIU7iIiKUjhLiKSgnwX7iu37OaxySsp2lOe7KqIiLRYvgv3nILdPDE1l+0lFcmuiohIi+W7cE8zA0D39RYRic534W7eY43SXUQkKv+Fu5fuynYRkeh8F+61x+4OpbuISDS+C3cduYuIxOe7cK89oSoiItH5Ltx1QlVEJD7/hbu6ZURE4vJvuCe3GiIiLZoPw732IibFu4hINP4Ld++xRtkuIhKV/8I9OFpG6S4iEo3/wt17VK+MiEh0vgv34MRhSa6HiEhL5rtwr+2VqVGnu4hIVP4Ld+9R0S4iEp3vwh1dxCQiEpfvwt00K6SISFy+C/c09cuIiMTlu3CvHeeu86kiItH5MNwDj+qWERGJLm64m1kfM5tmZivMbJmZ/TpCGTOzUWaWa2aLzWxQ01RXFzGJiCQiPYEyVcD/Oefmm1lHYJ6ZTXHOLQ8pcynQ3/sbAjztPTY600VMIiJxxT1yd85tds7N957vBlYAveoUuwJ4xQXMBrqYWc9Gry0hFzHp0F1EJKr96nM3syzgVGBOnZd6AXkhy/nU/wJoFMGb7CnbRUSiSjjczawD8B7wG+fcrrovR3hLvfg1s1vNLNvMsgsLC/evpvu24W1c6S4iEk1C4W5mGQSC/XXn3NgIRfKBPiHLvYFNdQs550Y75wY75wZnZmY2pL7Bce7qlRERiS6R0TIGvACscM49HqXYOOB6b9TMGUCxc25zI9ZzX33QOHcRkXgSGS1zNnAdsMTMFnrr7gT6AjjnngEmAkOBXKAUuKnxqxqw7wbZSncRkWjihrtzbiaR+9RDyzjgtsaqVCIU7SIi0fn3ClWlu4hIVL4L9zTdQ1VEJC7fhfu+i5iSWw8RkZbMf+FeO5+7wl1EJCr/hbtmhRQRict34a6LmERE4vNduBO8iEnpLiISje/C3WKOuBcREfBjuHuPOnAXEYnOf+GuWSFFROLyXbjrhKqISHy+C3fNCikiEp//wl2zQoqIxOW7cK+laBcRic534Z4W7HRPbj1ERFoy34V77VBIXcQkIhKd/8JdB+4iInH5L9w1K6SISFy+C/c0zQopIhKX78Id3axDRCQu34V7bbeM+mVERKLzX7jrhKqISFz+C3fvUQfuIiLR+S7c02pnhVS6i4hE5btwN51QFRGJy3/hXjvOPcn1EBFpyXwX7mhWSBGRuHwX7mm6h6qISFy+C/fa2+xp4jARkej8F+7eo7JdRCQ6/4W7LmISEYnLf+GuWSFFROLyX7hrVkgRkbj8G+7KdhGRqOKGu5m9aGYFZrY0yuvnmVmxmS30/kY2fjVD9oemHxARiSc9gTIvAU8Cr8QoM8M5d1mj1CgOHbmLiMQX98jdOTcd2N4MdUlIcOKwJNdDRKQla6w+9zPNbJGZfWRmxzfSNiOqHeeui5hERKJLpFsmnvnAkc65PWY2FHgf6B+poJndCtwK0Ldv3wbtTN0yIiLxHfCRu3Nul3Nuj/d8IpBhZt2jlB3tnBvsnBucmZnZoP2ZumVEROI64HA3s8PNS1wzO93b5rYD3W7sfaJDdxGRGOJ2y5jZm8B5QHczywfuBTIAnHPPAFcCvzCzKmAvMMw18ThFQzfrEBGJJW64O+eujvP6kwSGSjYbM9MVqiIiMfjuClUIHLmrV0ZEJDp/hrvphKqISCw+DXfTkbuISAz+DHc0t4yISCz+DHd1y4iIxOTPcMd05C4iEoMvwz3NNFpGRCQWX4a7mekiJhGRGPwZ7ug2eyIisfgy3FG3jIhITL4M99obdoiISGS+DHcz3axDRCQWf4Y76pYREYnFn+GuWSFFRGLyZ7ijI3cRkVj8Ge5mOm4XEYnBp+GuicNERGLxZ7ijbhkRkVj8Ge66iElEJCZfhnuaRsuIiMTky3A30MRhIiIx+DPcdZs9EZGYfBnuoFkhRURi8WW4W2DOXxERicKX4Z6mi5hERGLyZbhrVkgRkdj8Ge5onLuISCz+DHd1y4iIxOTTcNfcMiIisfgz3FG3jIhILP4Md00/ICISkz/DHR25i4jE4stwT9P0AyIiMfky3DXOXUQktrjhbmYvmlmBmS2N8rqZ2SgzyzWzxWY2qPGrWZ+iXUQkukSO3F8CLonx+qVAf+/vVuDpA69WbJoVUkQktrjh7pybDmyPUeQK4BUXMBvoYmY9G6uCkVigZk25CxERX2uMPvdeQF7Icr63rsmkpWm0jIhILI0R7hZhXcToNbNbzSzbzLILCwsPYIemE6oiIjE0RrjnA31ClnsDmyIVdM6Nds4Nds4NzszMbPAOzdQpIyISS2OE+zjgem/UzBlAsXNucyNsNypdxCQiElt6vAJm9iZwHtDdzPKBe4EMAOfcM8BEYCiQC5QCNzVVZUPqpCN3EZEY4oa7c+7qOK874LZGq1ECNCukiEhs/rxCFXXLiIjE4s9w16yQIiIx+TPc0ZG7iEgsvgx3zQopIhKbL8MdzQopIhKTL8Pd0EVMIiKx+DPcle4iIjH5MtzTNFpGRCQmX4Z74E5Mya6FiEjL5c9wx3SFqohIDP4Md80KKSISU9y5ZVqiGTlFya6CiEiL5ssj91plldXJroKISIvk63CfuKRJp40XEfEtX4e7zqmKiETm63AXEZHIfB3uOnAXEYnM1+EuIiKR+TrcdSGTiEhkvg53ERGJzNfhXqUJZkREIvJ1uI8YuyTZVRARaZF8He4Ar81en+wqiIi0OL4P97vfX5rsKoiItDi+D3cREalP4S4ikoIU7iIiKUjhLiKSghTuIiIpSOEuIpKCfBnuJ/XunOwqiIi0aL4M97uGHpfsKoiItGi+DPf0Vr6stohIs/FlSpoluwYiIi1bQuFuZpeY2UozyzWz4RFev9HMCs1soff388av6j6tdeQuIhJTerwCZtYKeAq4CMgH5prZOOfc8jpF33bO3d4EdaynTXp4uN/x9kLaZKTx0I9Oao7di4i0eIkcAp8O5Drn1jjnKoC3gCuatlqxpaWF98uMXbCRN7/K052ZREQ8iYR7LyAvZDnfW1fXj81ssZm9a2Z9GqV2UbSK0un+u3cWN+VuRUR8I5Fwj5SkdQ+RPwSynHMnAZ8AL0fckNmtZpZtZtmFhYX7V9MQ3Tu2ibj+vfn5Dd6miEgqSSTc84HQI/HewKbQAs65bc65cm/xOeC0SBtyzo12zg12zg3OzMxsSH0B6NAm7qkCEZGDWiLhPhfob2b9zKw1MAwYF1rAzHqGLF4OrGi8KoqIyP6KewjsnKsys9uBSUAr4EXn3DIzuw/Ids6NA35lZpcDVcB24MYmrLOIiMSRUP+Gc24iMLHOupEhz0cAIxq3aiIi0lAH5dVAeyuqqaiqSXY1RESaTMqF+3UvzKFoTzk1NdHHvB838mOGjprRjLUSEWleKTfsZEZOEYMf+IQzj+rGbecfTWVNDecfe1i9crkFe5JQOxGR5pFy4V5r1pptzFqzDYB1D38vybUREWleKdctIyIiB1m45+8oJWv4hGRXQ0SkyR1U4X7bGwuSXQURkWZxUIX7orydYcv3frA0STUREWlaB1W41/XyrPX8Z4EmGxOR1HNQhPvNL82NOtf7b99exL0fLOXRSSubuVYiIk3noAj3qV8X0G/ExKivvzxrPU9Oy23GGomINK2DItwTlTV8AiXlVcmuhojIAfNtuM+/56Im2e7GnXubZLsiIs3Jt+HeHPdL/XDRJkZ9mtPk+2ls+TtKOe6ej8nZujvZVRGRJPFvuDfDPn755gIen7Kq3vrNxXt5Y86GZqhBw3y0ZAt7K6t5e25e/MIikpL8G+5NlO4X/306i/N3UlW9b0rgrOETKN5bGVy+4cWvuPM/S9hRUtE0lYhi0P1T+OvHXydcvjm+AEWkZfJvuDdhdP3mrYUcfddHYeuuHj07+Hy7F+rV3jfM799ZxHUvzAkrX1xaGXPa4YbYXlLBvz5b3ajbFJHU5Ntwb0prikrqrVu+eRdlldURy78zL58ZOUXBPu5te8o5+b7J/OOT8C6djTv3Ut3IgR+LHcB7V23dzdoI/w4i4g++DfdWdiDR1TAD7vmYyuoaQvN52sqC4POfPDsLgG3ekf2oqbnBI/q87aWc/fBU/vlJ/T785jR9VSGzvamQY7n479M5/9HPGnXfldU1TP16a6NuU0Qi8224d+vQhgd/eAKv3nJ6s+63/10fBbtlVmzexU3/nht8bUdpZb3yM3KKANi6qwyAL1bHDtYZOYVNGoDXv/gVw0K6mCJpqlE2oz7N4eaXspm8bEuTbF9E9vFtuANcO+RIvtU/M2n7jxTm8YQO4Vy/rYRVW3fz8dLN5G0vBeC6F77i5peyefnLdWEndZvLl7lFXPT36U2y7cnLAl9at2t2TpEmlxJ3Yvr2MZl8vqqw2ff7qzfrh9S89Tto37pV2LrQOeTnbwjMTOmc49t/+yy4vlWa0e2Q1sHle8ctY8P2Uu65bGCj1PWL3CL6dG0fXC6vCgyV/NmQI0lL29fFtboJ+9lrRxxVJOFLS+RgkxLh/tz1gznm7o/iF2wGP376y7hlbntjPuf27x62rrrGUbC7PGzdCzPXcuNZWXRqm8HJ903e77qEnrq99vnw0TxPfJrLk9Ny6dg2nR+e2nu/t90QNc1w4ZmIBPi6W6ZW6/Q0nr3utGRXI2ETFm/mmc/XJFT2W3+dRt6O0kavw47SwHmDPeWRRwDVyl63vUHdQ6sL91BaET5PTyIDhT5bWcBnISepRaRhUiLcAb57/OFcPLBHsqvRoiQ0nsi54GRpJeVV9UbSXPnMrP2egqGmxnHhY5/z36/Oq7Orfem+JL844pfGjf+ey40hJ6lFpGFSJtwBRl19KpN/e26yq5GQ/RlDnlMQPnqloqqGpRuLo5Z/f+FGAJ6fuZY7xiyMWOZ1b/qEJ6bmcvy9k9heUsEvXp/PhMWb65WtPZ9RWlEVdqVuNLXdL1/kBkYKrSncQ9bwCcEhogDff3JmxKkdGsuW4jLGLdrUJNv+Mreo3l29RFqalAr3thmt6Hto+/gFfea3by8KW/7LxBVc9sRM1m8LfEH84d1FnPXQp8HXl23aFXw+dv5Glocs11Xbzz9hyWamRzkpvbsscGR//qOfcfKfA33/5VXVfL0l8nbr9r58GWX457JNuxi/eBNZwydQsLssah0b4qejZ/GrNxfwm7cWBH+ZjJ2fz8CRgWsVDsQ1z8/hiqe+aIxqNtjO0goW5+sLRqJLqXAHSAu5uCmzYxvW/GVoEmvTNF76cl3Y45jsfDYVRw/HoaNmxN3mPe/HuJ+s90+6dVd5sOyxd3/MJf+Ywebi+lMkh543zRo+gbujbHtXWWVwWOTUFQVsidGG/bV5Z2Bb7y/cxCuz1gNw3/jllFZUB7+sElVZXcPfp6yKOde/c44vc4uaZbZSgGGjZ3P5k8n9gpH98+LMtXy5uqjZ9pdy4d46PY0/XjIACGRS6DC/VPPvL9YxK+SouKq6hqI95THe0TBlFdX88d3FweVXZ68PPt/jBWV5VTUFu8soKa9irzdNQ7wTqKFh+ebcPM4I+fVxoEKHWz7y8dfBLiKA/yzYWG8aiFdnreOhiSvIGj6B4e8tpryqOqz8Pz/NidmN9NqcDVzz/BwmLmnYBVon/WkSIxO8Yfv2kgq+3uLf6Zxn5hSxtyL2ifxUdN/45Vzz3Jz4BRtJyoU7wI8G9Up2FZrN1c/tu9r06Ls+4qGJic8amahNxWW8nR15+uBF+YG+/9+8tZDTH/yU4++dFOy6iaf26Bqo14f9+JRVwSuBAcYv3sQPnvqCiqqGdamEDgW9f/xyXp21Luz1ez5YxrPTAyOY3pqbx2hvNFNpRRWFXtdVWWU1f/5wWcTtr/fOoWzybvaypbiMsfP33Xx9Z2kFn68qpKbG1euC2lNexa6yquAvjHhWbI7ezXagampcQtNTRHrf6sI9ccutLSrhZy/MYcTYxXHLNpZv3DmRYaNnNdv+WoqUDPdatcdmaQY/GdybW889Kuz1kZcN5LnrBzd/xZrQeyGB0hx+984ilm4s5qOl+3/EujtGN8eoT3MYdP8UrnhyJseP/Jjb31jAwrydHHP3R9TUOHaUVPDopJXkFuzGOZdQd8jOkCuKQ0/uzsipf65hjzeM85xHpvE37+bpZZU1/PuLdRG3/eIXa8OWr3r2S+4Ys4jHJgfee8p9U7jhxa/49duBL8EN20pxzjHt6wJOuHdS8H1vz418n4CPlmyO+KtsW4xfasV7Kxn5wdKwCe8qq2uYv2FHcHnpxuKwL4vnZ65h2OjZYXMmJeLpz1dz4WOfxzy/A/t+6b2/cFNw1tSNO/cGT9TX1Li424i43fIqsoZPYEyEexhU1zhmr9nO4AemkFsQ/wsoVaTERUx1dW6XAcBNZ2cBsOah7wVf++UFR/PA+BXMWrONm8/pB8DMP57POY9Ma/Z6porLnpjZZNuu/WUQ6uaX5/LZykAgvzMvj2MP78T0VYWs/stQnpiak1Cf+iuz1rNgw05m5kbuA62uDvShh/56qPvFOX/DDhZu2MnN5/Sr1wWVtz1wBP/E1FwuGHBYcP2H3giec/82jaEnHs7KOt0rD05YwU+/2Tds3e6ySn7x+nxO6NWJ8b/8Vtg5jdMe+IR1D3+PSG5/Yz4zcoqYtGwLc+78DgB//fhrnpuxlpGXDeS+8cuDZb99TCZnfaNbcBTX/p7/mL8+8IWRt6OUAYd3DOsOfWpaLpOXb+Xa0/syoGfHsPW/vLA/Zz88lcM7tWX2nRfy9Oer+duklbx/29mc0qdLwvuvDe2nPsvl/AGH0bV9BmlmYfUo2lPBa7PX86fLj9+vtu2v4tJKapyja8gV59FmlG1KKRnubTNaRf3Ad2ybwSNXnhS2rnfX9vxz2Cn07tqeO8cuYaVuT9ei1QY7BE7ybt0VWL7ppblRR/zUVby3MmqwQ2AY6fMz10Z9HeBH/wpcjXxMj32BNSO3iM7tM8LK/fBfka9ajtQ/v8v7YnLOcff7S/nJ4D4c2S0wAmzpxl3c+Z8lDD2hZ9h7soZPYM6dF9KjU9uw9fsmrSvnoyWbufTEnsGRVKHBDoHhrnWn8FiYt5OBPTvROj3wA7+kvIpnp6/hrG90Y2dpBYOO7MphHcP3+d+vzuP4Izox4VffYtuectq1bhX85bMobyfjbj87WPaxKauCv6637Crj1dnrgyOANu3cGxbuX+YW8ft3F/PJHd+mnTe9R1llNW3S08gt2MMPvNFL67eV8s0HPwHgeyf25LiQL5PGsL2kgkH3T+GNnw/hrKO7RyxTezX5h7efw4m9O1Nd43hl1rrg62WV1bTNaBXxvY3Jmuvsfl2DBw922dnZSdl3PKFzwQCccdSh5G3fyyu3nM6Fj32epFrJweSvPz6JP7wX6Jf+8PZz+P6T+34dPf6Tk7ljTPjw2OevH8x3BvZgV1klJ/1pMhmtjMrqff9vX3lab9LTjLcSvPXif32rH8/NWMvJvTtz1eA+/GhQLx6bvIoXQr7wendtx8w/XsA5j0wlf0f4qKl1D3+PrOETOKr7IWH3R3jtliH87IX4JxX/de0ghp6470vssidmsHTjLh696mSuPK13MGS/mdWVuet2xNhSZPPvuYhDD2lNztbdrC4s4ZITDk/ofSPGLuHNrzZw1je68cZ/nRGxTGh+LLr34nrnoGr/3RrKzOY55+L2Jyd05G5mlwD/BFoBzzvnHq7zehvgFeA0YBvwU+fcuv2tdEvx2i1DGLdoIx8v3cKxh3fk1VuGkNEqcPTSsW06u8uq+NGgXoydvzHJNZVUVRvsQFiwA/WCHSAjPY3C3eX8zDtxHBrsAO/O279zMbWjcRblF7Mov5hPV2ylZ5d2YWXyd+xlzNy8esEOgS4XqH/jm0SCHeCVWes4tW8XurRrTVpa4FcLBM7x/HhQr+BJ6YYEOwRuWfmrC45m1NRAPW86O4t7v388n68qZOQHS/nhqb246ax+WBp0apvB0XdOpEPb9OB5m72V1VRW1wRzwTnH8PeWcE6dOaNue31+vX1H+vdqCnGP3M2sFbAKuAjIB+YCVzvnloeU+V/gJOfc/5jZMOCHzrmfxtpuSz5yj2XDtlIW5O3gilN6saW4jDMe+pTO7TJYdO/FzFmzjUMPac1Ff5/Oyb0788Ht57CjpIJT758ScVs5D17KeX/7jI07m+c/tqSuozIPYU2h7px1IC4YcBhTv96/E8m1vzB+8dq8/RpUsPahoVgDbziU6JF7IuF+JvAn59x3veURAM65h0LKTPLKzDKzdGALkOlibNyv4V5X7YmS0D60Dxdt4sxvdKN7hzZhZacs38ptb8ynoqqGUVefyuUnH8Gjk1by5LRcHr3qZJ6cmsO6bfsmCUsz6Nf9EDq3ywhOFSwiqSHaecF4GrNbphcQ2lGXDwyJVsY5V2VmxUA3oPkux0qSSCdGvn/yERHLXjSwB6seuDRs3W8vOoYrT+tNVvdDuGDAYXywcCM3npVV71t95ZbdVFbX0L9HB9qkh+/zkn9M5/ozs7hmSF827tzLhMWbuGDAYbRJb8X6baXsKa+ktKKadUUlXH7KEYDxncc/p3uH1vzxkgGc3u9QxmTn8dS01Tx1zSC2l5RzwXE9uPHFr2jXuhWLI4xYaUx9Dm3HpSf0ZPT0xGbKFJH4Ejlyvwr4rnPu597ydcDpzrlfhpRZ5pXJ95ZXe2W21dnWrcCtAH379j1t/frELtqQ5Ao9u19RVRMcPQGBcclbd5fRs/O+/tg95VW0TU8jvVX0yygqqmpolWYszNvBoL5dw77MNmwr5YgubSnYXU63Dq1ZU1hCZsc2dGmXwf3jl3PNkCM59vCO7CytIKdgD8WllRzeuS2HdWzD5uIyOrfLIKv7IZRWVLEkv5i0NGNgz06kmVGwu4zJy7Zy/oBMSsqrOal3Zxbm7eSUPl3YU17Fq7PXc/HAHjw4YQX9e3TkxF6deXdePredfzTdO7Rm/fZSdpRUcOFxPfgit4icrXs4pkcH7hiziAuOO4zKqhqO7NaeHaWVrCsq4V8/G8SYuXk8OnkVn/3uPPoc2p6VW3bz7rx83snO48azs1hTWMIRXdpy7ZAj+fOHy5jmjQbq1/0Q8raXcuu5R/HCzLV0bJvOgMM7BUf5XDOkL1XVNYzJjtyffkKvTpSWV0e84TsEunLat27FtUOOZMTYJQl+GuIzC5+Coq4Te3VmycZiHvjBCTw2eWWD7mgWzQ9OOYL3F9afMK57hzbUOBc2tDWZpv/+fPp2a9g8WOqWERFJQYmGeyJXqM4F+ptZPzNrDQwDxtUpMw64wXt+JTA1VrCLiEjTitvn7vWh3w5MIjAU8kXn3DIzuw/Ids6NA14AXjWzXGA7gS8AERFJkoTGuTvnJgIT66wbGfK8DLiqcasmIiINldITh4mIHKwU7iIiKUjhLiKSghTuIiIpSOEuIpKCkjblr5kVAg29RLU7qTO1gdrSMqVKW1KlHaC21DrSOZcZr1DSwv1AmFl2Ildo+YHa0jKlSltSpR2gtuwvdcuIiKQghbuISArya7iPTnYFGpHa0jKlSltSpR2gtuwXX/a5i4hIbH49chcRkRh8F+5mdomZrTSzXDMbnuz6RGJmL5pZgZktDVl3qJlNMbMc77Grt97MbJTXnsVmNijkPTd45XPM7IZI+2ridvQxs2lmtsLMlpnZr33clrZm9pWZLfLa8mdvfT8zm+PV621vWmvMrI23nOu9nhWyrRHe+pVm9t3mbotXh1ZmtsDMxvu8HevMbImZLTSzbG+d7z5fXh26mNm7Zva19//MmUlti3PON38EphxeDRwFtAYWAQOTXa8I9TwXGAQsDVn3V2C493w48Ij3fCjwEWDAGcAcb/2hwBrvsav3vGszt6MnMMh73pHAjdIH+rQtBnTwnmcAc7w6jgGGeeufAX7hPf9f4Bnv+TDgbe/5QO9z1wbo530eWyXhM3YH8AYw3lv2azvWAd3rrPPd58urx8vAz73nrYEuyWxLsza+Ef7xzgQmhSyPAEYku15R6ppFeLivBHp6z3sCK73nzwJX1y0HXA08G7I+rFyS2vQBcJHf2wK0B+YTuBdwEZBe9/NF4P4FZ3rP071yVvczF1quGevfG/gUuAAY79XLd+3w9ruO+uHuu88X0AlYi3cesyW0xW/dMpFu1t0rSXXZXz2cc5sBvMfDvPXR2tSi2ur9nD+VwBGvL9vidWUsBAqAKQSOVnc656oi1Cvspu9A7U3fW0Jb/gH8Aajxlrvhz3YAOGCymc2zwD2WwZ+fr6OAQuDfXnfZ82Z2CElsi9/C3SKs8/twn2htajFtNbMOwHvAb5xzu2IVjbCuxbTFOVftnDuFwJHv6cBxkYp5jy2yLWZ2GVDgnJsXujpC0RbdjhBnO+cGAZcCt5nZuTHKtuS2pBPoin3aOXcqUEKgGyaaJm+L38I9H+gTstwbqH+r85Zpq5n1BPAeC7z10drUItpqZhkEgv1159xYb7Uv21LLObcT+IxAX2cXC9zUvW69gnX2Xu9M4BaSyW7L2cDlZrYOeItA18w/8F87AHDObfIeC4D/EPjS9ePnKx/Id87N8ZbfJRD2SWuL38I9kZt1t1ShNxG/gUD/de36672z52cAxd7Pt0nAxWbW1TvDfrG3rtmYmRG4P+4K59zxmLzcAAABK0lEQVTjIS/5sS2ZZtbFe94O+A6wAphG4KbuUL8tkW76Pg4Y5o1C6Qf0B75qnlaAc26Ec663cy6LwOd/qnPuWnzWDgAzO8TMOtY+J/C5WIoPP1/OuS1Anpkd6626EFhOMtvS3CdQGuHExVACozZWA3cluz5R6vgmsBmoJPBNfAuBfs5PgRzv8VCvrAFPee1ZAgwO2c7NQK73d1MS2nEOgZ+Ei4GF3t9Qn7blJGCB15alwEhv/VEEQi0XeAdo461v6y3neq8fFbKtu7w2rgQuTeLn7Dz2jZbxXTu8Oi/y/pbV/v/sx8+XV4dTgGzvM/Y+gdEuSWuLrlAVEUlBfuuWERGRBCjcRURSkMJdRCQFKdxFRFKQwl1EJAUp3EVEUpDCXUQkBSncRURS0P8DaqTkcCtjcJQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the FNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to training the nerual network, we also need to load batches of test images and collect the outputs. The differences are that:\n",
    "(1) No loss & weights calculation\n",
    "(2) No wights update\n",
    "(3) Has correct prediction calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10K test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = torch.FloatTensor(images.view(-1, 28*28))\n",
    "    \n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "    total += labels.size(0)                    # Increment the total count\n",
    "    correct += (predicted == labels).sum()     # Increment the correct count\n",
    "    \n",
    "print('Accuracy of the network on the 10K test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained FNN Model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(net.state_dict(), 'fnn_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
