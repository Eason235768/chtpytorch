{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784       # The image size = 28 x 28 = 784\n",
    "hidden_size = 500      # The number of nodes at the hidden layer\n",
    "num_classes = 10       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 5         # The number of times entire dataset is trained\n",
    "batch_size = 100       # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,), (0.3081,))])\n",
    "train_dataset = dsets.MNIST(root='./data',\n",
    "                           train=True,\n",
    "                           transform=transform,\n",
    "                           download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data',\n",
    "                           train=False,\n",
    "                           transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.3860, -0.1951,\n",
       "           -0.1951, -0.1951,  1.1795,  1.3068,  1.8032, -0.0933,  1.6887,\n",
       "            2.8215,  2.7197,  1.1923, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.0424,  0.0340,  0.7722,  1.5359,  1.7396,  2.7960,\n",
       "            2.7960,  2.7960,  2.7960,  2.7960,  2.4396,  1.7650,  2.7960,\n",
       "            2.6560,  2.0578,  0.3904, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "            0.1995,  2.6051,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "            2.7960,  2.7960,  2.7960,  2.7706,  0.7595,  0.6195,  0.6195,\n",
       "            0.2886,  0.0722, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.1951,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.7960,\n",
       "            2.0960,  1.8923,  2.7197,  2.6433, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242,  0.5940,  1.5614,  0.9377,  2.7960,  2.7960,  2.1851,\n",
       "           -0.2842, -0.4242,  0.1231,  1.5359, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.2460, -0.4115,  1.5359,  2.7960,  0.7213,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242,  1.3450,  2.7960,  1.9942,\n",
       "           -0.3988, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.2842,  1.9942,  2.7960,\n",
       "            0.4668, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0213,  2.6433,\n",
       "            2.4396,  1.6123,  0.9504, -0.4115, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.6068,\n",
       "            2.6306,  2.7960,  2.7960,  1.0904, -0.1060, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "            0.1486,  1.9432,  2.7960,  2.7960,  1.4850, -0.0806, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.2206,  0.7595,  2.7833,  2.7960,  1.9560, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242,  2.7451,  2.7960,  2.7451,  0.3904,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "            0.1613,  1.2305,  1.9051,  2.7960,  2.7960,  2.2105, -0.3988,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,  0.0722,  1.4596,\n",
       "            2.4906,  2.7960,  2.7960,  2.7960,  2.7578,  1.8923, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.1187,  1.0268,  2.3887,  2.7960,\n",
       "            2.7960,  2.7960,  2.7960,  2.1342,  0.5686, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.1315,  0.4159,  2.2869,  2.7960,  2.7960,  2.7960,\n",
       "            2.7960,  2.0960,  0.6068, -0.3988, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.1951,\n",
       "            1.7523,  2.3633,  2.7960,  2.7960,  2.7960,  2.7960,  2.0578,\n",
       "            0.5940, -0.3097, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242,  0.2758,  1.7650,  2.4524,\n",
       "            2.7960,  2.7960,  2.7960,  2.7960,  2.6815,  1.2686, -0.2842,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242,  1.3068,  2.7960,  2.7960,\n",
       "            2.7960,  2.2742,  1.2941,  1.2559, -0.2206, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "           -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242]]]),\n",
       " tensor(5))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shuffle the loading process of train_dataset to make the learning process independent of data orderness, but the order of test_loader remains to examine whether we can handle unspecified bias order of inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network Model Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "    \n",
    "        self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the FNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0296, -0.0272,  0.0228,  ...,  0.0134,  0.0126,  0.0108],\n",
      "        [-0.0247, -0.0070,  0.0005,  ..., -0.0240, -0.0092, -0.0204],\n",
      "        [ 0.0304,  0.0337,  0.0348,  ..., -0.0294, -0.0074, -0.0203],\n",
      "        ...,\n",
      "        [-0.0178, -0.0179,  0.0204,  ...,  0.0127, -0.0245, -0.0010],\n",
      "        [-0.0281,  0.0272, -0.0091,  ...,  0.0187,  0.0198,  0.0170],\n",
      "        [ 0.0316, -0.0244,  0.0308,  ...,  0.0283, -0.0276,  0.0040]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0336, -0.0126,  0.0268, -0.0143,  0.0203, -0.0255,  0.0226, -0.0199,\n",
      "        -0.0319, -0.0322,  0.0071, -0.0313,  0.0312,  0.0108,  0.0348,  0.0264,\n",
      "         0.0040,  0.0287, -0.0186,  0.0242, -0.0066, -0.0252,  0.0128, -0.0174,\n",
      "         0.0260,  0.0327, -0.0029,  0.0286, -0.0297, -0.0342,  0.0342, -0.0188,\n",
      "        -0.0088, -0.0298, -0.0067,  0.0281,  0.0189,  0.0323,  0.0074,  0.0172,\n",
      "        -0.0164, -0.0331, -0.0079, -0.0197, -0.0049,  0.0105,  0.0322, -0.0017,\n",
      "         0.0312,  0.0021,  0.0198, -0.0192,  0.0019, -0.0327,  0.0053,  0.0251,\n",
      "        -0.0335, -0.0279,  0.0183,  0.0173, -0.0342, -0.0176, -0.0332,  0.0115,\n",
      "        -0.0094, -0.0022, -0.0082,  0.0265,  0.0267,  0.0290, -0.0130,  0.0257,\n",
      "        -0.0111, -0.0199, -0.0163, -0.0057, -0.0238, -0.0269, -0.0232, -0.0165,\n",
      "         0.0199, -0.0146,  0.0189,  0.0016, -0.0175,  0.0342,  0.0075, -0.0032,\n",
      "        -0.0260, -0.0347,  0.0281,  0.0009, -0.0081, -0.0168, -0.0277, -0.0117,\n",
      "         0.0269, -0.0100, -0.0186, -0.0235,  0.0064, -0.0153, -0.0208,  0.0296,\n",
      "         0.0357, -0.0104, -0.0318,  0.0172,  0.0127,  0.0131, -0.0112,  0.0099,\n",
      "        -0.0343,  0.0006,  0.0347,  0.0085, -0.0273, -0.0167,  0.0292, -0.0214,\n",
      "         0.0004,  0.0056,  0.0006, -0.0088,  0.0044, -0.0184,  0.0281, -0.0205,\n",
      "        -0.0261,  0.0356, -0.0155, -0.0085, -0.0238, -0.0223, -0.0058,  0.0153,\n",
      "        -0.0279,  0.0345, -0.0073,  0.0336,  0.0238,  0.0268, -0.0239, -0.0045,\n",
      "         0.0113,  0.0276,  0.0255,  0.0088,  0.0146, -0.0031, -0.0043,  0.0079,\n",
      "        -0.0131,  0.0207, -0.0122,  0.0081, -0.0063, -0.0045, -0.0161, -0.0335,\n",
      "         0.0114, -0.0209,  0.0255,  0.0020,  0.0305, -0.0205, -0.0009,  0.0332,\n",
      "        -0.0351,  0.0310, -0.0085,  0.0061, -0.0290,  0.0309, -0.0090,  0.0039,\n",
      "         0.0312,  0.0201,  0.0126, -0.0198,  0.0039,  0.0206,  0.0271,  0.0126,\n",
      "         0.0093, -0.0149, -0.0146,  0.0061,  0.0239, -0.0158,  0.0327, -0.0073,\n",
      "         0.0126,  0.0283, -0.0034, -0.0298, -0.0281,  0.0063,  0.0032, -0.0055,\n",
      "         0.0272, -0.0286,  0.0349, -0.0186,  0.0168,  0.0043,  0.0039, -0.0311,\n",
      "         0.0057, -0.0060,  0.0044,  0.0294, -0.0285,  0.0248, -0.0193, -0.0071,\n",
      "        -0.0249,  0.0112,  0.0207,  0.0127, -0.0036, -0.0237,  0.0225, -0.0000,\n",
      "        -0.0033,  0.0167, -0.0008,  0.0121,  0.0227, -0.0102,  0.0135, -0.0114,\n",
      "        -0.0043,  0.0151,  0.0300,  0.0330, -0.0297, -0.0073, -0.0316,  0.0193,\n",
      "        -0.0154,  0.0254, -0.0242, -0.0013,  0.0328, -0.0284, -0.0216,  0.0266,\n",
      "         0.0298, -0.0102, -0.0005,  0.0110,  0.0098, -0.0315,  0.0008, -0.0227,\n",
      "        -0.0066, -0.0146, -0.0242, -0.0252, -0.0134,  0.0230,  0.0053,  0.0169,\n",
      "         0.0324, -0.0054,  0.0060, -0.0335,  0.0297,  0.0345,  0.0114, -0.0200,\n",
      "        -0.0290,  0.0119, -0.0209, -0.0336, -0.0228,  0.0090,  0.0183, -0.0325,\n",
      "         0.0261, -0.0060,  0.0193, -0.0217,  0.0111,  0.0093,  0.0238, -0.0013,\n",
      "        -0.0288,  0.0027,  0.0158, -0.0216,  0.0348, -0.0345,  0.0066, -0.0180,\n",
      "        -0.0010, -0.0049,  0.0223, -0.0023,  0.0183,  0.0202,  0.0179,  0.0124,\n",
      "         0.0087, -0.0267,  0.0155, -0.0225,  0.0060,  0.0200, -0.0147,  0.0021,\n",
      "        -0.0017,  0.0126,  0.0212, -0.0144,  0.0025, -0.0305,  0.0126, -0.0002,\n",
      "         0.0279,  0.0178,  0.0270, -0.0330,  0.0035,  0.0302, -0.0255,  0.0259,\n",
      "        -0.0326, -0.0075,  0.0068, -0.0182, -0.0213,  0.0262, -0.0342,  0.0217,\n",
      "        -0.0310,  0.0133,  0.0106, -0.0319,  0.0235,  0.0213, -0.0078,  0.0137,\n",
      "        -0.0178,  0.0263,  0.0244,  0.0074, -0.0228,  0.0191, -0.0066,  0.0081,\n",
      "        -0.0222,  0.0145,  0.0230, -0.0126, -0.0174,  0.0174, -0.0133, -0.0039,\n",
      "         0.0271, -0.0146,  0.0005, -0.0057,  0.0153, -0.0209, -0.0041, -0.0236,\n",
      "         0.0288,  0.0273, -0.0249, -0.0014,  0.0319, -0.0107, -0.0312,  0.0083,\n",
      "        -0.0116, -0.0059, -0.0300,  0.0223,  0.0193, -0.0350,  0.0105, -0.0246,\n",
      "         0.0055,  0.0276, -0.0008,  0.0116, -0.0133,  0.0202,  0.0298, -0.0173,\n",
      "         0.0175,  0.0010,  0.0325, -0.0298,  0.0275,  0.0217,  0.0318, -0.0263,\n",
      "         0.0056,  0.0131, -0.0318,  0.0109, -0.0229,  0.0189, -0.0329,  0.0338,\n",
      "        -0.0239, -0.0015,  0.0169,  0.0096, -0.0326,  0.0034,  0.0140,  0.0037,\n",
      "         0.0288, -0.0082, -0.0135,  0.0178,  0.0124, -0.0122,  0.0142,  0.0207,\n",
      "         0.0210, -0.0110, -0.0015,  0.0321, -0.0239,  0.0338, -0.0278,  0.0134,\n",
      "         0.0185,  0.0314, -0.0161,  0.0164,  0.0148, -0.0226, -0.0207,  0.0176,\n",
      "         0.0341, -0.0153,  0.0225,  0.0095, -0.0232,  0.0307,  0.0134, -0.0212,\n",
      "        -0.0079,  0.0107, -0.0158,  0.0289, -0.0289, -0.0212, -0.0091, -0.0266,\n",
      "        -0.0144,  0.0139, -0.0159,  0.0218,  0.0242, -0.0342, -0.0350,  0.0116,\n",
      "         0.0062,  0.0205, -0.0339, -0.0192,  0.0019, -0.0349,  0.0088,  0.0070,\n",
      "         0.0295,  0.0348, -0.0125, -0.0165, -0.0141, -0.0055,  0.0170, -0.0190,\n",
      "         0.0065, -0.0120,  0.0058, -0.0015,  0.0089,  0.0119, -0.0235,  0.0316,\n",
      "        -0.0180, -0.0018, -0.0239, -0.0052, -0.0122,  0.0302, -0.0097,  0.0146,\n",
      "         0.0246, -0.0309, -0.0298,  0.0309], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0297,  0.0289, -0.0321,  ...,  0.0227, -0.0012, -0.0231],\n",
      "        [ 0.0045,  0.0355,  0.0055,  ...,  0.0441,  0.0178,  0.0142],\n",
      "        [ 0.0330,  0.0437,  0.0390,  ..., -0.0357,  0.0194, -0.0171],\n",
      "        ...,\n",
      "        [-0.0343,  0.0402,  0.0186,  ..., -0.0082,  0.0219, -0.0243],\n",
      "        [ 0.0271, -0.0040, -0.0427,  ..., -0.0016,  0.0383, -0.0356],\n",
      "        [ 0.0040, -0.0015, -0.0437,  ...,  0.0431,  0.0062, -0.0226]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0434, -0.0353,  0.0139,  0.0315,  0.0367,  0.0438,  0.0135, -0.0303,\n",
      "         0.0390, -0.0126], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "a=net.parameters()\n",
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose the Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0077, -0.0129, -0.0203,  ..., -0.0071,  0.0048, -0.0113],\n",
      "        [ 0.0221,  0.0278, -0.0158,  ..., -0.0030, -0.0351,  0.0140],\n",
      "        [-0.0031, -0.0132, -0.0318,  ...,  0.0301,  0.0068,  0.0027],\n",
      "        ...,\n",
      "        [-0.0138, -0.0271,  0.0112,  ...,  0.0178, -0.0179, -0.0045],\n",
      "        [ 0.0075, -0.0049, -0.0309,  ..., -0.0153, -0.0017,  0.0258],\n",
      "        [-0.0199, -0.0189,  0.0356,  ...,  0.0145, -0.0321,  0.0105]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0197,  0.0056,  0.0133,  0.0271, -0.0103,  0.0226,  0.0043,  0.0167,\n",
      "         0.0245, -0.0119,  0.0138, -0.0346,  0.0303, -0.0024,  0.0020,  0.0345,\n",
      "         0.0302, -0.0020, -0.0247,  0.0289,  0.0246,  0.0018,  0.0145, -0.0038,\n",
      "        -0.0336, -0.0269,  0.0186,  0.0357, -0.0038,  0.0312, -0.0349,  0.0294,\n",
      "         0.0194,  0.0242, -0.0019, -0.0251, -0.0331,  0.0058,  0.0257, -0.0345,\n",
      "        -0.0111, -0.0028, -0.0177, -0.0305,  0.0335,  0.0085,  0.0033,  0.0220,\n",
      "         0.0290,  0.0353, -0.0191,  0.0158, -0.0304,  0.0334, -0.0184,  0.0216,\n",
      "        -0.0195, -0.0072,  0.0323,  0.0287,  0.0107, -0.0029, -0.0016, -0.0038,\n",
      "         0.0147, -0.0296, -0.0194,  0.0061,  0.0309, -0.0073, -0.0110, -0.0211,\n",
      "         0.0083, -0.0039, -0.0309,  0.0310, -0.0289,  0.0033, -0.0310, -0.0046,\n",
      "         0.0040, -0.0355, -0.0284,  0.0044, -0.0029, -0.0173,  0.0079,  0.0283,\n",
      "         0.0064,  0.0072,  0.0151, -0.0257, -0.0079,  0.0172, -0.0188,  0.0034,\n",
      "         0.0226,  0.0000, -0.0338, -0.0236,  0.0297,  0.0274, -0.0092, -0.0145,\n",
      "        -0.0250, -0.0240,  0.0312, -0.0257, -0.0043,  0.0182, -0.0357, -0.0274,\n",
      "        -0.0185, -0.0141,  0.0149, -0.0160, -0.0212, -0.0031, -0.0326, -0.0326,\n",
      "         0.0076, -0.0249,  0.0328, -0.0021, -0.0025,  0.0252, -0.0094, -0.0158,\n",
      "         0.0265, -0.0348, -0.0132,  0.0190, -0.0057, -0.0153, -0.0217, -0.0251,\n",
      "         0.0272,  0.0253, -0.0226,  0.0293, -0.0014,  0.0280, -0.0074,  0.0096,\n",
      "        -0.0135,  0.0168,  0.0053, -0.0090, -0.0251,  0.0108,  0.0282, -0.0050,\n",
      "         0.0098, -0.0277,  0.0274, -0.0066,  0.0183,  0.0056,  0.0276, -0.0080,\n",
      "        -0.0004,  0.0139,  0.0344, -0.0211, -0.0240, -0.0142,  0.0048,  0.0248,\n",
      "        -0.0267,  0.0179, -0.0198,  0.0269,  0.0120, -0.0336, -0.0050, -0.0071,\n",
      "        -0.0050, -0.0015, -0.0286, -0.0330, -0.0222, -0.0301,  0.0271, -0.0002,\n",
      "        -0.0098,  0.0121,  0.0081, -0.0020,  0.0236,  0.0100,  0.0101,  0.0264,\n",
      "        -0.0349, -0.0064, -0.0148, -0.0244, -0.0001, -0.0209, -0.0032,  0.0072,\n",
      "        -0.0220, -0.0000, -0.0177, -0.0149, -0.0295,  0.0057,  0.0180, -0.0022,\n",
      "        -0.0176, -0.0014,  0.0014, -0.0288, -0.0136, -0.0238,  0.0284, -0.0252,\n",
      "         0.0142, -0.0104, -0.0169,  0.0169,  0.0203,  0.0242, -0.0274,  0.0087,\n",
      "         0.0071, -0.0354, -0.0106,  0.0178, -0.0070,  0.0104,  0.0252, -0.0194,\n",
      "        -0.0047, -0.0220,  0.0188, -0.0060, -0.0006,  0.0284, -0.0121, -0.0155,\n",
      "         0.0228, -0.0204, -0.0326, -0.0278, -0.0228, -0.0205,  0.0259, -0.0350,\n",
      "         0.0332, -0.0156, -0.0089,  0.0266,  0.0116,  0.0089, -0.0079,  0.0119,\n",
      "        -0.0067,  0.0179,  0.0305, -0.0331,  0.0048,  0.0189,  0.0260, -0.0196,\n",
      "        -0.0225,  0.0206, -0.0191, -0.0260, -0.0163, -0.0236, -0.0280,  0.0076,\n",
      "         0.0038, -0.0291, -0.0040,  0.0149,  0.0075,  0.0241, -0.0081, -0.0136,\n",
      "         0.0106,  0.0086, -0.0297,  0.0025,  0.0273,  0.0325,  0.0049,  0.0111,\n",
      "         0.0014,  0.0110, -0.0266, -0.0100,  0.0048,  0.0239, -0.0237,  0.0011,\n",
      "         0.0222,  0.0253,  0.0122, -0.0350,  0.0186,  0.0223,  0.0118, -0.0276,\n",
      "         0.0346, -0.0258,  0.0276,  0.0313, -0.0180, -0.0323,  0.0169,  0.0304,\n",
      "        -0.0110,  0.0062,  0.0155, -0.0048,  0.0290, -0.0086,  0.0341, -0.0056,\n",
      "        -0.0109, -0.0176,  0.0199, -0.0284, -0.0304, -0.0311,  0.0343,  0.0036,\n",
      "         0.0253,  0.0177,  0.0332,  0.0106, -0.0090, -0.0347,  0.0153,  0.0342,\n",
      "         0.0021,  0.0244, -0.0041,  0.0311, -0.0204,  0.0174, -0.0344, -0.0040,\n",
      "        -0.0126,  0.0230, -0.0224, -0.0088, -0.0073, -0.0072,  0.0307,  0.0028,\n",
      "         0.0208,  0.0327,  0.0141, -0.0327,  0.0211,  0.0292,  0.0105,  0.0193,\n",
      "        -0.0190, -0.0081, -0.0100,  0.0224, -0.0143, -0.0164, -0.0337, -0.0101,\n",
      "        -0.0006,  0.0108,  0.0208, -0.0143,  0.0184, -0.0100, -0.0079, -0.0175,\n",
      "         0.0257, -0.0079,  0.0005,  0.0068, -0.0288,  0.0159, -0.0023, -0.0139,\n",
      "         0.0085, -0.0349,  0.0063,  0.0353, -0.0144,  0.0326,  0.0073, -0.0071,\n",
      "        -0.0017, -0.0113, -0.0308, -0.0255,  0.0168, -0.0123, -0.0145, -0.0287,\n",
      "         0.0341, -0.0009, -0.0033,  0.0091, -0.0089,  0.0195, -0.0110, -0.0087,\n",
      "        -0.0106,  0.0073,  0.0013, -0.0003,  0.0200, -0.0133, -0.0140, -0.0283,\n",
      "        -0.0143, -0.0068, -0.0284, -0.0125,  0.0283, -0.0221, -0.0037,  0.0261,\n",
      "         0.0119,  0.0132,  0.0308, -0.0114,  0.0192, -0.0120,  0.0195,  0.0333,\n",
      "        -0.0229,  0.0045,  0.0230, -0.0127,  0.0028, -0.0292, -0.0092,  0.0162,\n",
      "         0.0115,  0.0229,  0.0143, -0.0024, -0.0192,  0.0141, -0.0296,  0.0012,\n",
      "         0.0153,  0.0114, -0.0100,  0.0010, -0.0096,  0.0187, -0.0129, -0.0351,\n",
      "        -0.0236, -0.0287, -0.0264,  0.0144, -0.0303, -0.0319, -0.0002,  0.0151,\n",
      "         0.0175,  0.0181, -0.0307, -0.0087, -0.0055, -0.0004,  0.0309, -0.0301,\n",
      "        -0.0003,  0.0241,  0.0201, -0.0187, -0.0280,  0.0040,  0.0090,  0.0133,\n",
      "        -0.0282, -0.0045, -0.0160,  0.0269, -0.0084,  0.0057,  0.0078,  0.0012,\n",
      "         0.0342,  0.0140, -0.0156, -0.0127, -0.0047,  0.0034, -0.0071, -0.0110,\n",
      "         0.0037,  0.0173,  0.0328,  0.0032], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0271,  0.0284, -0.0141,  ..., -0.0103, -0.0400,  0.0150],\n",
      "        [-0.0226, -0.0088,  0.0014,  ..., -0.0299,  0.0344, -0.0305],\n",
      "        [ 0.0343,  0.0123,  0.0202,  ..., -0.0394, -0.0212,  0.0013],\n",
      "        ...,\n",
      "        [ 0.0067,  0.0027,  0.0411,  ..., -0.0259, -0.0367,  0.0003],\n",
      "        [ 0.0151, -0.0433,  0.0150,  ..., -0.0145, -0.0309,  0.0167],\n",
      "        [-0.0281, -0.0175,  0.0228,  ..., -0.0294,  0.0272, -0.0202]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0310, -0.0071, -0.0242,  0.0170, -0.0159, -0.0210,  0.0166, -0.0075,\n",
      "        -0.0367, -0.0234], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in net.parameters():\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the FNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshhu/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:10: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/home/joshhu/anaconda3/envs/pytorch/lib/python3.6/site-packages/ipykernel/__main__.py:16: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.2110\n",
      "Epoch [1/5], Step [200/600], Loss: 0.2499\n",
      "Epoch [1/5], Step [300/600], Loss: 0.1495\n",
      "Epoch [1/5], Step [400/600], Loss: 0.1820\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1330\n",
      "Epoch [1/5], Step [600/600], Loss: 0.0339\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0674\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0801\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0382\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0579\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1993\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0531\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0366\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0244\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0703\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0592\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0342\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0595\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0089\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0266\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0509\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0214\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0876\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0492\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0088\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0353\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0107\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0524\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0232\n",
      "Epoch [5/5], Step [600/600], Loss: 0.1319\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):   # Load a batch of images with its (index, data, class)\n",
    "        images = torch.FloatTensor(images.view(-1, 28*28))         # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = torch.LongTensor(labels)\n",
    "        \n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = net(images)                             # Forward pass: compute the output class given a image\n",
    "        loss = criterion(outputs, labels)                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        losses.append(loss.data[0])\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "        \n",
    "        if (i+1) % 100 == 0:                              # Logging\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the FNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to training the nerual network, we also need to load batches of test images and collect the outputs. The differences are that:\n",
    "(1) No loss & weights calculation\n",
    "(2) No wights update\n",
    "(3) Has correct prediction calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10K test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = torch.FloatTensor(images.view(-1, 28*28))\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "    total += labels.size(0)                    # Increment the total count\n",
    "    correct += (predicted == labels).sum()     # Increment the correct count\n",
    "    \n",
    "print('Accuracy of the network on the 10K test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained FNN Model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(net.state_dict(), 'fnn_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fcd0fb57320>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot(losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
